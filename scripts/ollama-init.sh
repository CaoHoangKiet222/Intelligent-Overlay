#!/bin/sh

set -e

OLLAMA_HOST="${OLLAMA_HOST:-http://localhost:11434}"

echo "ğŸš€ Starting Ollama model preloader..."
echo "OLLAMA_HOST: $OLLAMA_HOST"

# Models Ä‘Æ°á»£c chá»n tá»‘i Æ°u cho GPU 4GB:
# - phi3:mini: Model chÃ­nh cho generation tasks, nháº¹ (~2.3GB), cháº¥t lÆ°á»£ng tá»‘t cho general purpose
# - qwen2.5:1.5b: Model cháº¥t lÆ°á»£ng cao cho cÃ¡c tasks phá»©c táº¡p hÆ¡n, tá»‘t cho reasoning vÃ  quality tasks
# - qwen2.5:0.5b: Model siÃªu nháº¹ cho classification vÃ  simple tasks, ráº¥t nhanh
# - bge-micro: Embedding model nhá» gá»n (384 dim), phÃ¹ há»£p cho retrieval vÃ  semantic search
MODELS="phi3:mini bge-micro"

wait_for_ollama() {
  echo "â³ Waiting for Ollama to be ready..."
  i=1
  while [ $i -le 30 ]; do
    if curl -s "$OLLAMA_HOST/api/tags" > /dev/null 2>&1; then
      echo "âœ… Ollama is ready!"
      return 0
    fi
    echo "   Attempt $i/30..."
    i=$((i + 1))
    sleep 2
  done
  echo "âŒ Ollama did not become ready in time"
  return 1
}

pull_model() {
  model=$1
  echo ""
  echo "ğŸ“¥ Pulling model: $model"
  if curl -X POST "$OLLAMA_HOST/api/pull" -d "{\"name\": \"$model\"}" -H "Content-Type: application/json" --no-buffer 2>&1 | grep -q '"status":"success"'; then
    echo "âœ… Successfully pulled: $model"
    return 0
  else
    echo "âš ï¸  Failed to pull $model, continuing..."
    return 1
  fi
}

wait_for_ollama

for model in $MODELS; do
  pull_model "$model"
done

echo ""
echo "ğŸ‰ All models preloaded successfully!"
echo ""
echo "ğŸ“‹ Available models:"
curl -s "$OLLAMA_HOST/api/tags" || echo "Could not list models"

